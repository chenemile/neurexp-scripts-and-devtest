# -*- coding: utf-8 -*-
'''
:author: Emily Chen
:date:   2021

USAGE:  python2.7 get_neural_wer.py /nas/data/yupik/finite_state_morphology/uppercase.fomabin devtest/src-dev.txt devtest/tgt-dev.txt [PREDICTIONS.TXT]  [N_BEST]

'''
import argparse
import pprint

from sys import exit
from foma import *


parser = argparse.ArgumentParser()

parser.add_argument('fst', help='file path to fst analyzer')
parser.add_argument('input', help='file path to words to be analyzed')
parser.add_argument('pred', help='file path to predicted analyses')
parser.add_argument('gold', help='file path to gold analyses')
parser.add_argument('nbest', help='num n-best predictions made')
parser.add_argument('logreg', help='file path to logistic regression csv')
args = parser.parse_args()

# populate following lists
gold_analyses      = []
predicted_analyses = []
words              = []

with open(args.gold, 'r') as f:
    for line in f:
        analysis = line.replace(" ","").strip().decode('utf-8')
        gold_analyses.append(analysis)

with open(args.pred, 'r') as f:
    n = 1
    predictions = []
    for line in f:
        if n == int(args.nbest) + 1:
            predicted_analyses.append(predictions)
            
            # reset the n-best count
            n = 1
            predictions = []

        analysis = line.replace(" ","").strip().decode('utf-8')
        predictions.append(analysis)
        n += 1

    # don't forget to add last n-best predictions
    predicted_analyses.append(predictions)

with open(args.input, 'r') as f:
    for line in f:
        word = line.replace(" ","").strip().decode('utf-8')
        words.append(word)


# prepare variables for logistic regression csv
distribution2num = {"random":   1,
                    "mixed":    2,
                    "shortest": 3,
                    "unifrac":  4,
                    "condfrac": 5}

sampling2num = {"1A": 1,
                "2A": 2,
                "2B": 3,
                "3A": 4}

distribution = (args.pred).rsplit("/", 1)[1].rsplit("-", 2)[0]
distribution_option = distribution2num[distribution]

sampling = (args.pred).rsplit("/", 1)[1].rsplit("-", 2)[1]
sampling_option = sampling2num[sampling]

size_option = (args.pred).rsplit("/", 1)[1].rsplit("-", 1)[1].split("mil")[0]

if len(gold_analyses) != len(predicted_analyses):
    print(len(gold_analyses))
    print(len(predicted_analyses))
    print("WARNING: number of gold standard items does not match the " + \
          "number of items being analyzed")
    exit()
else:
    t = FST.load(args.fst)

    num_tokens_wrong = 0
    num_tokens = len(gold_analyses)

    tokens_wrong = []

    with open(args.logreg, 'a') as lg:

        # make csv file to assist with manual error analysis
        #   WORD, PREDICTED, ANALYSIS 1, ... , ANALYSIS [n_best]
        with open('manual_error_analysis.csv', 'w') as csvfile:
    
            for i in range(len(gold_analyses)):
                match = False
        
                for prediction in predicted_analyses[i]:
                    if gold_analyses[i].lower() == prediction.lower():
                        match = True
                        break
        
                    # if gold analysis does not match predicted analysis,
                    # compare their surface forms generated by the FST analyzer
                    # (possible syncretism)
                    else:
                        gold_analysis = gold_analyses[i][0].lower() + gold_analyses[i][1:]
        
                        if prediction:
                            predicted_analysis = prediction[0].lower() + prediction[1:]
        
                        gold_words = t[gold_analysis]
                        predicted_words = t[predicted_analysis]
        
                        sf_in_common = set(gold_words).intersection(predicted_words)
        
                        if len(sf_in_common) > 0:
                            match = True
                            break
        
                #  outputs all errors (tokens)
                if not match:
                    num_tokens_wrong += 1
                    tokens_wrong.append("item ID " + str(i+1) + ": " + words[i])
        
                    # update error analysis csv
                    guesses = ""
                    for j in range(int(args.nbest)):
                        tmp = predicted_analyses[i][j].encode('utf-8') + ","
                        guesses += tmp
        
                    csvfile.write(words[i].encode('utf-8') + "," + \
                                  gold_analysis.encode('utf-8') + "," + \
                                  guesses + \
                                  "\n")

                # update logistic regression csv
                if match:
                    lg.write(str(i) + ", " + str(1) + ", " + str(distribution_option) + ", " + str(sampling_option) + ", " + str(size_option) + "\n")
                else:
                    lg.write(str(i) + ", " + str(0) + ", " + str(distribution_option) + ", " + str(sampling_option) + ", " + str(size_option) + "\n")
        
    num_types = len(set(gold_analyses))
    types_wrong = [tok.split(": ")[1] for tok in tokens_wrong]
    num_types_wrong = len(set(types_wrong))

    print("===================")
    print("neural analyzer WER") 
    print("===================")
        
    print("----------------")
    print("problem children")
    print("----------------")
    pprint.pprint(tokens_wrong)

    print()

    print("------")
    print("types")
    print("------")
    print("  acc = {:.2f}".format((num_types - num_types_wrong)/float(num_types) * 100))

    print("------")
    print("tokens")
    print("------")
    print("  acc = {:.2f}".format((num_tokens - num_tokens_wrong)/float(num_tokens) * 100))
